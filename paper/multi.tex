\section{``Worse is Better'' and the Need for Multipath}
\label{sec:multi}

In his widely read article \emph{The Rise of ``Worse is Better''} Richard Gabriel offered a novel theory of why technically imperfect software systems often gain traction and widespread acceptance, even where more sophisticated systems fail \cite{worseisbetter}. Consider two systems: the first is simple to implement, but sacrifices completeness, consistency, and sometimes even correctness in service of this simplicity. The second is created to be completely correct, consistent, and complete, but sacrifices simplicity to this end. The first system (using what Gabriel calls the ``New Jersey approach'') is considerably ``worse'' than the second (the ``MIT approach''): it is unreliable, limited in what it can do, and imposes extra burdens on the user. However, it will be created faster, ported more easily, and by the time the second system has been perfected the first will already have been widely adopted. Thus, Gabriel argues, the New Jersey approach will generally create more successful technologies than the MIT approach. C is a classic example of the New Jersey approach; Lisp is an example of the MIT approach. C was so successful not because it was technically superior to Lisp, but because it was much easier to write compilers for. 

The Internet is one of the best examples of the New Jersey approach to design. Crucial to the widespread adoption of the Internet is the simplicity of the core protocols. Each piece of functionality is shunted as far up the network stack as possible: for example, instead of relying on the Link Layer or the Network Layer to guarantee reliable delivery, it's left to the Transport Layer. While this may be inefficient, because packets must be sent across the entire network when a loss occurs on any link, it makes switches and routers far simpler and cheaper to implement. Similarly, instead of explicitly allocating bandwidth to each connection, as in telephone networks, the Internet uses statistical multiplexing, relying on Transport level congestion avoidance algorithms to ensure an approximately fair allocation of resources. In fact, the well known end-to-end principle\cite{saltzer1984end} can be viewed as an application of ``Worse is Better'' to networking: lower layers in the network stack should be as simple as possible, leaving all possible functionality to the higher layers.

\subsection{The Resource Pooling Principle}
\label{sec:pooling}
While the ``Worse is Better'' approach may explain the Internet's widespread adoption, modern web applications demand reliablity, consistency, and scalability that the TCP/IP stack cannot provide on its own. This need has given rise to a wide array of technologies that address these issues from the Application Layer, including load balancers, network address translators (NATs), and content distribution networks (CDNs). For each of these systems, all or part of their functionality consists of taking care of problems which would have been addressed within the network, had the Internet had been designed with the MIT approach in mind. Now, this is not necessarily a bad thing: many of these technologies address needs which could not have been forseen when TCP/IP was designed, and solving these problems at the Application Layer allows for flexibility and quick deployment. 

However, as Wischik argues in \cite{wischik2008resource}, these technologies all use variations of a single technique, \emph{resource pooling}. The basic idea is to make multiple heterogenous resources behave like a larger single resource, for improved efficiency, capacity, and reliability. For example, a load balancer spreads traffic across a number of servers, providing the abstraction of a single server with much greater procesessing power and bandwidth. The problem is that when this technique is implemented over and over in every part of the network, massive redundancy and complexity is created. Wischik states the problem as his \emph{resource pooling principle}: ``Resource pooling is such a powerful tool that designers at every part of the network will attempt to build their own load-shifting mechanisms. A network architecture is effective overall, only if these mechanisms do not conflict with each other.''\cite{wischik2008resource} 

Wischik's key insight that motivates the development of a multipath transport protocol is this: many forms of resource pooling currently performed by diverse Application Layer technologies can be handled elegantly and efficiently by a multipath-capable Transport Layer. When a single connection can simultaneously utilize multiple links, it is effectively pooling the capacity of all of these links. If one link fails, traffic can simply be sent over the others. When one link is heavily congested, instead of backing off transmission altogether, more traffic can be shifted to uncongested links. By having two paths go through different ISPs, one can even reap the benefits of multihoming without putting pressure on BGP.