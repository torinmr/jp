\section{The Need for Multipath}
\label{sec:multi}

In his widely read article \em{The Rise of ``Worse is Better''}\em{}\cite{worseisbetter}, Richard Gabriel offered a novel theory of why technically imperfect software systems often gain traction and widespread acceptance, even where more sophisticated systems fail. Consider two systems: the first is simple to implement, but sacrifices completeness, consistency, and sometimes even correctness in service of this simplicity. The second is created to be completely correct, consistent, and complete, but sacrifices simplicity to this end. The first system is considerably ``worse'' than the second: it is unreliable, limited in what it can do, and imposes extra burdens on the user. However, it will be created faster, ported more easily, and by the time the second system has been perfected the first will already have been adopted by everybody. C is an example of the first type of system; Lisp is an example of the second. C was so successful not because it was technically superior to Lisp, but because it was much easier to write compilers for. 

The Internet is one of the best examples of a ``Worse is Better'' system. Crucial to the widespread adoption of the Internet is the simplicity of the core protocols. Each piece of functionality is shunted as far up the network stack as possible: for example, instead of relying on the Link Layer or the Network Layer to guarantee reliable delivery, it's left to the Transport Layer. While this may be inefficient, because packets must be sent across the entire network when a loss occurs on any link, it makes switches and routers far simpler and cheaper to implement. Similarly, instead of explicitly allocating bandwidth to each connection, as in telephone networks, the Internet uses statistical multiplexing, relying on Transport level congestion avoidance algorithms to ensure an approximately fair allocation of resources.

However, while ``Worse is Better'' may be sufficient most of the time,